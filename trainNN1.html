<!DOCTYPE html>
<html>
<body>

<p>xxxxx</p>

<button onclick="recTest()">Try it</button>

<p id="sintan"></p>
<p id="sintanv"></p>

<p id="sintanser"></p>
<p id="sintanserv"></p>

<script>
"use strict";
// example data generation
/*var N = 100; // number of points per class
var D = 2; // dimensionality
var K = 3; // number of classes
var X = Mat(N*K,D); // data matrix (each row = single example), X: matrix(100 * 3, 2)
var y = zeros(N*K); // class labels, here 100 * 3
for ( var j=0; j<K; j++) {
	var ix = range(N*j,N*(j+1),0); // generates a list of numbers, range([start], stop[, step])
	var r = range(0,1,N); // radius; linspace: return evenly spaced numbers over a specified interval
	var t = range(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 // theta
	X[ix] = np.c_[r*np.sin(t), r*np.cos(t)] // np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])] -> array([[1, 2, 3, 0, 0, 4, 5, 6]])
	y[ix] = j
}
// lets visualize the data:
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)

// initialize parameters randomly

// W: matrix(2,100), W2: matrix (100,3), b: vector(100), b2: vector(3)

var h = 100; // size of hidden layer
// D=2 is the dimensionality (pic), K=3 is the number of classes
var W = 0.01 * RandMat(D,h, 0.01, 1.0); // Return a sample from the “standard normal” distribution, d0, d1,... the dimensions of the returned array
var b = zeros(1,h);
var W2 = 0.01 * RandMat(h,K, 0.01, 1.0);
var b2 = zeros(1,K);

// some hyperparameters
var step_size = 1;
var reg = 0.001; // regularization strength

// gradient descent loop
var num_examples = X.shape[0]; // X.shape is (n,m), so this is n
for ( var i=0; i<10; i++) { // was 10000
	// evaluate class scores, [N x K]
	// h = X . W + b
	var hidden_layer = mMax(0, mAdd(dot(X, W), b)); // note, ReLU activation; np.maximum([2, 3, 4], [1, 5, 2]) -> array([2, 5, 4]) hidden_layer: vector(100)
	// scores = h . W2 + b2
	var scores = mAdd(dot(hidden_layer, W2), b2); // will be a vector(3)

	// compute the class probabilities
	var exp_scores = scores.exp(); // element-wise exp, will be a vector(3); Calculate the exponential of all elements in the input array
	var probs = exp_scores.div(mSum(exp_scores, 1)); // [N x K] = 100 x 3; np.sum([[0, 1], [0, 5]], axis=1) -> array([1, 5]) (array([0, 6]) for axis=0); will be a vector(3)

	// compute the loss: average cross-entropy loss and regularization
	correct_logprobs = -np.log(probs[range(0, num_examples, 0),y]) // range(start, end, step), range(n) generates a list of 10 values; Natural logarithm, element-wise
	data_loss = mSum(correct_logprobs, -1)/num_examples;
	reg_loss = 0.5*reg*mSum(mMult(W, W), -1) + 0.5*reg*mSum(mMult(W2, W2), -1); // sum here just sums all array elements
	loss = data_loss + reg_loss;
	if ( i % 1000 == 0 )
		console.log("iteration " + i + " : loss " + loss);

	// compute the gradient on scores
	dscores = probs;
	dscores[range(0, num_examples, 0),y] -= 1;
	dscores /= num_examples;

	// backpropagate the gradient to the parameters
	// first backprop into parameters W2 and b2
	dW2 = dot(mTrans(hidden_layer), dscores); // .T = transpose
	db2 = mSum(dscores, 0);
	// next backprop into hidden layer
	dhidden = dot(dscores, mTrans(W2));
	// backprop the ReLU non-linearity
	dhidden[hidden_layer <= 0] = 0
	// finally into W,b
	dW = dot(mTrans(X), dhidden);
	db = sum(dhidden, 0);

	// add regularization gradient contribution
	dW2 = mSum(dW2, mCMult(reg, W2));
	dW = mSum(dW, mCMult(reg, W));

	// perform a parameter update
	W = mSum(W, mCMult(-step_size, dW);
	b = mSum(b, mCMult(-step_size, db);
	W2 = mSum(W2, mCMult(-step_size, dW2);
	b2 = mSum(b2, mCMult(-step_size, db2);
}*/

function recTest() {
// input dataset :: example from iamtrask.github: http://iamtrask.github.io/2015/07/12/basic-python-network/
	/*X = np.array([  [0,0,1],
					[0,1,1],
					[1,0,1],
					[1,1,1] ]); */
	var X = new Mat(5,3); 	// Inputs L0: Input dataset matrix where each row is a training example, each column corresponds to one of our input nodes
	X.set(0,0,0);			// so there are 3 input nodes and 4 training examples
	X.set(1,0,0);
	X.set(2,0,1);
	X.set(3,0,1);
	X.set(0,1,0);
	X.set(1,1,1);
	X.set(2,1,0);
	X.set(3,1,1);
	X.set(0,2,1);
	X.set(1,2,1);
	X.set(2,2,1);
	X.set(3,2,1);
	
	X.set(4,0,1);
	X.set(4,1,1);
	X.set(4,2,1);
		
	// output dataset            
	//y = np.array([[0,1,1,0]]).T;
	var y = new Mat(5,1); // Outputs L2: Output dataset matrix where each row is a training example
	y.set(0,0,0);
	y.set(1,0,0.5);
	y.set(2,0,0.5);
	y.set(3,0,0);
	
	y.set(4,0,1);
	
	//console.log(JSON.stringify(X));
	//console.log(JSON.stringify(y));
	
	// inputs L0 -> hidden weights L1 -> outputs L2

	// initialize weights randomly with mean 0
	var syn0 = RandMat(3,5,-0,5); // first layer of weights, Synapse 0, connecting l0 to l1
	var syn1 = RandMat(5,1,0,5); // Second layer of weights, Synapse 1 connecting l1 to l2
	//console.log(JSON.stringify(syn1));

	for (var iter=0; iter<50000; iter++) { // network training code
		// forward propagation: feed forward through layers 0, 1, and 2
		var l0 = mCopy(X); // First Layer of the Network, specified by the input data
		
		// hidden weights L1 
		// sigmoid-shaped response to input data l0, l1 times current weights syn0
		var l1s = dot(l0, syn0);
		var l1 = mSig(l1s, false); // Second Layer of the Network, otherwise known as the hidden layer: prediction step or guess
		var l2s = dot(l1, syn1);
		var l2 = mSig(l2s, false); // Final Layer of the Network, which is our hypothesis, and should approximate the correct answer as we train
		//console.log("l1: " + JSON.stringify(l1));
		var l2_error = mSub(y, l2); // This is the amount that the neural network "missed"
		//console.log("l2: " + JSON.stringify(l2));
		//console.log("l2_error: " + JSON.stringify(l2_error));
		
		if (iter % 10000 == 0) console.log("Error: " + l2_error.aavrg());
		
		// This is the error of the network scaled by the confidence. It's almost identical to the error except that very confident errors are muted.
		var l2sig = mSig(l2, true);
		var l2_delta = mMult(l2_error, l2sig); // in what direction is the target value? were we really sure? if so, don't change too much
		
		// how much did we miss? distance from desired output is the error
		// Weighting l2_delta by the weights in syn1, we can calculate the error in the middle/hidden layer.
		//console.log("syn1: " + JSON.stringify(mTrans(syn1)));
		//console.log("l2 delta: " + JSON.stringify(l2_delta));
		//var l1_error = dot(l2_delta, mTrans(syn1)); // how much did each l1 value contribute to the l2 error (according to the weights)? backpropagation
		var l1_error = mMult(l2_delta, mTrans(syn1));
		//console.log("t l1err: " + typeof l1_error);
		//console.log("l1_error: " + JSON.stringify(l1_error));

		// multiply how much we missed by the slope of the sigmoid at the values in l1: the Error Weighted Derivative
		// in what direction is the target l1?
		// This is the l1 error of the network scaled by the confidence. Again, it's almost identical to the l1_error except that confident errors are muted.
		l1s = mSig(l1, true);
		var l1_delta = mMult(l1_error, mSig(l1, true)); // network is confident (i.e. very high or low value) for "clear" predictions,
		// i.e. the ones that are far from x=0; it is not confident in the middle, the slope is steep, the derivative high, thise estimates
		// are updated heavily; this is working a bit like a switch: it rather flips than being continually adjusted like a dimmer
		//console.log("l1_delta: " + JSON.stringify(l1_delta));

		// update weights: weight_update = input_value * l1_delta
		syn1 = mAdd(syn1, dot(mTrans(l1), l2_delta));
		syn0 = mAdd(syn0, dot(mTrans(l0), l1_delta));
		//console.log("syn1: " + JSON.stringify(syn1));
		//console.log("syn0: " + JSON.stringify(syn0));
	}

	console.log("Output After Training: " + JSON.stringify(l2));
}


function sigmoid(x,deriv) { 
    if( deriv )
        return x*(1-x);
    return 1 / (1 + 1 / Math.exp(x));
	//var x3 = x*x*x;
	//var x5 = x3*x*x;
	//return ( 0.5 + x/4 - x3/48 + x5/480 );
}

function randRg(min, max) {
    return Math.random() * (max - min) + min;
}

// Standard Normal variate using Box-Muller transform, mean 0, variance 1
function randn_bm() {
	var u = 1 - Math.random(); // Subtraction to flip [0, 1) to (0, 1].
	var v = 1 - Math.random();
	return Math.sqrt( -2.0 * Math.log( u ) ) * Math.cos( 2.0 * Math.PI * v );
}

// Random numbers utils
var return_v = false;
var v_val = 0.0;
var gaussRandom = function() {
	if(return_v) { 
		return_v = false;
		return v_val; 
	}
		
	var u = 2*Math.random()-1;
	var v = 2*Math.random()-1;
	var r = u*u + v*v;
	if(r == 0 || r > 1) return gaussRandom();
	var c = Math.sqrt(-2*Math.log(r)/r);
	v_val = v*c; // cache this
	return_v = true;
	return u*c;
}

var randf = function(a, b) { return Math.random()*(b-a)+a; }
var randi = function(a, b) { return Math.floor(Math.random()*(b-a)+a); }
var randn = function(mu, std){ return mu+gaussRandom()*std; }

// helper function returns array of zeros of length n
// and uses typed arrays if available
var zeros = function(n) {
	if(typeof(n)==='undefined' || isNaN(n)) { return []; }
	if(typeof ArrayBuffer === 'undefined') {
		// lacking browser support
		var arr = new Array(n);
		for(var i=0;i<n;i++) { arr[i] = 0; }
		return arr;
	} else {
		return new Float64Array(n);
	}
}

// Mat holds a matrix
var Mat = function(n,d) {
	// n is number of rows d is number of columns
	this.n = n;
	this.d = d;
	this.w = zeros(n * d);
	this.dw = zeros(n * d);
}

Mat.prototype = {
	get: function(row, col) { 
		// slow but careful accessor function
		// we want row-major order
		var ix = (this.d * row) + col;
		return this.w[ix];
	},
	set: function(row, col, v) {
		// slow but careful accessor function
		var ix = (this.d * row) + col;
		this.w[ix] = v; 
	},
	toJSON: function() {
		var json = {};
		json['n'] = this.n;
		json['d'] = this.d;
		json['w'] = this.w;
		return json;
	},
	fromJSON: function(json) {
		this.n = json.n;
		this.d = json.d;
		var nd = this.n * this.d;
		this.w = zeros(nd);
		this.dw = zeros(nd);
		for ( var i=0,n=nd; i<n; i++) {
			this.w[i] = json.w[i]; // copy over weights
		}
	},
	getrows: function() {
		return this.n;
	},
	getcols: function() {
		return this.d;
	},
	exp: function() {
		var nd = this.n * this.d;
		for ( var i=0; i<nd; i++ ) {
			this.w[i] = Math.exp(this.w[i]);
		}
	},
	log: function() {
		var nd = this.n * this.d;
		for ( var i=0; i<nd; i++ ) {
			this.w[i] = Math.log(this.w[i]);
		}
	},
	div: function(a) {
		if ( a == 0 ) return -1;
		var nd = this.n * this.d;
		for ( var i=0; i<nd; i++ ) {
			this.w[i] = this.w[i] / a;
		}
	},
	aavrg: function() {
		var nd = this.n * this.d;
		var av = 0;
		for ( var i=0; i<nd; i++ ) {
			av += ~~this.w[i];
		}
		return av / nd;
	}
}

// return Mat but filled with random numbers from gaussian
function RandMat(r,c,mu,std) {
	var m = new Mat(r, c);
	//fillRandn(m,mu,std);
	fillRand(m,-std,std); // kind of :P
	return m;
}

// Mat utils
// fill matrix with random gaussian numbers
var fillRandn = function(m, mu, std) { for(var i=0,n=m.w.length;i<n;i++) { m.w[i] = randn(mu, std); } }
var fillRand = function(m, lo, hi) { for(var i=0,n=m.w.length;i<n;i++) { m.w[i] = randf(lo, hi); } }

function mCopy(a) { // copy a
	var aNumRows = a.getrows(), aNumCols = a.getcols(),
      m = new Mat(aNumRows, aNumCols);  // initialize new matrix
	for (var r = 0; r < aNumRows; ++r) {
		for (var c = 0; c < aNumCols; ++c) {
			m.set(r,c, a.get(r,c));
		}
	}
	return m;
}

function mMult(a, b) {
	var aNumRows = a.getrows(), aNumCols = a.getcols(),
      bNumRows = b.getrows(), bNumCols = b.getcols(),
      m = new Mat(aNumRows, bNumCols);  // initialize new matrix
	for (var r = 0; r < aNumRows; ++r) {
		for (var c = 0; c < bNumCols; ++c) {
			m.set(r,c,0); // initialize the current cell
			for (var i = 0; i < aNumCols; ++i) {
				m.set(r,c, m.get(r,c) + a.get(r,i) * b.get(i,c));
			}
		}
	}
	return m;
}

function mCMult(a, b) { // const a, matrix b
	var bNumRows = b.getrows(), bNumCols = b.getcols(),
      m = new Mat(bNumRows, bNumCols);  // initialize new matrix
	for (var r = 0; r < bNumRows; ++r) {
		for (var c = 0; c < bNumCols; ++c) {
			m.set(r,c, a * b.get(r,c));
		}
	}
	return m;
}

function mAdd(a, b) { // a + b
	var aNumRows = a.getrows(), aNumCols = a.getcols(),
      bNumRows = b.getrows(), bNumCols = b.getcols(),
      m = new Mat(aNumRows, aNumCols);  // initialize new matrix
	for (var r = 0; r < aNumRows; ++r) {
		for (var c = 0; c < bNumCols; ++c) {
			m.set(r,c, a.get(r,c) + b.get(r,c));
		}
	}
	return m;
}

function mSub(a, b) { // a - b
	var aNumRows = a.getrows(), aNumCols = a.getcols(),
      bNumRows = b.getrows(), bNumCols = b.getcols(),
      m = new Mat(aNumRows, aNumCols);  // initialize new matrix
	for (var r = 0; r < aNumRows; ++r) {
		for (var c = 0; c < bNumCols; ++c) {
			m.set(r,c, a.get(r,c) - b.get(r,c));
		}
	}
	return m;
}

function dot(a, b) {
	var aNumRows = a.getrows(), aNumCols = a.getcols(),
      bNumRows = b.getrows(), bNumCols = b.getcols();
	if ( aNumCols != bNumRows && aNumRows != bNumCols ) return 0;
	if ( aNumCols == 1 || aNumRows == 1 ) {
	    var m = 0;
		for (var r = 0; r < aNumRows; ++r) {
			for (var c = 0; c < bNumCols; ++c) {
				for (var i = 0; i < aNumCols; ++i) {
					m += a.get(r,i) * b.get(i,c);
					//console.log("r: " + r + " c: " + c + " i: " + i + " m: " + m);
				}
			}
		}	
	} else {
	    var m = new Mat(aNumRows, bNumCols);  // initialize new matrix
		for (var r = 0; r < aNumRows; ++r) {
			for (var c = 0; c < bNumCols; ++c) {
				m.set(r,c,0); // initialize the current cell
				for (var i = 0; i < aNumCols; ++i) {
					m.set(r,c, m.get(r,c) + a.get(r,i) * b.get(i,c));
				}
			}
		}
	}
	return m;
}

function mSum(a, axis) {
	var aNumRows = a.getrows(), aNumCols = a.getcols();
	if ( axis == 0 ) { // sum colums
		var m = new Mat(aNumCols, 1);  // initialize new vector
		for (var c = 0; c < aNumCols; ++c) {
			for (var r = 0; r < aNumRows; ++r) {
				m.set(c,0, m.get(c,0) + a.get(r,c));
			}
		}
	} else if ( axis == 1 ) { // sum rows
		var m = new Mat(1, aNumRows);  // initialize new vector
		for (var r = 0; r < aNumRows; ++r) {
			for (var c = 0; c < aNumCols; ++c) {
				m.set(0,r, m.get(0,r) + a.get(r,c));
			}
		}
	} else { // sum entire matrix
		var m = new Mat(1, 1);  // initialize new vector
		for (var r = 0; r < aNumRows; ++r) {
			for (var c = 0; c < aNumCols; ++c) {
				m.set(0,0, m.get(0,0) + a.get(r,c));
			}
		}
	}
	return m;
}

function range(start, stop, inc) {
	var len = stop - start;
	var m = new Mat(1, len);  // initialize new vector
	for (var i = start; i < stop; i++) {
		if ( inc > 0 ) m.set(0, i - start, i / inc );
		else m.set(0, i - start, i );
	}
	return m;
}

function log(vec) {
	var len = vec.getcols();
	var m = new Mat(1, len);  // initialize new vector
	for (var i = 0; i < len; i++) {
		m.set(0,i, Math.log(vec.get(0,1)));
	}
	return m;
}

function combVec(a, b) {
	var lena = a.getcols();
	var lenb = b.getcols();
	var len = lena + lenb;
	var m = new Mat(1, len);  // initialize new vector
	for (var i = 0; i < len; i++) {
		if ( i >= lena ) m.set(0,i, b.get(0, i - lena));
		else m.set(0,i, a.get(0,i));
	}
	return m;
}

function mTrans(a) { // transpose
	var aNumRows = a.getrows(), aNumCols = a.getcols(),
      m = new Mat(aNumCols, aNumRows);  // initialize new matrix
	for (var r = 0; r < aNumRows; ++r) {
		for (var c = 0; c < aNumCols; ++c) {
			m.set(c,r, a.get(r,c));
		}
	}
	return m;
}

function mMax(a, b) { // maximum: a = a number, b = a matrix (element-wise)
	var bNumRows = b.getrows(), bNumCols = b.getcols(),
      m = new Mat(bNumRows, bNumCols);  // initialize new matrix
	for (var r = 0; r < bNumRows; ++r) {
		for (var c = 0; c < bNumCols; ++c) {
			m.set(r,c, Math.max(a, b.get(r,c)));
		}
	}
	return m;
}

function mSig(a, deriv) { // sigmoid on every element of a and return as matrix
	var aNumRows = a.getrows(), aNumCols = a.getcols(),
      m = new Mat(aNumRows, aNumCols);  // initialize new matrix
	for (var r = 0; r < aNumRows; ++r) {
		for (var c = 0; c < aNumCols; ++c) {
			m.set(r,c, sigmoid(a.get(r,c), deriv));
		}
	}
	return m;
}

/*
N = 100 // number of points per class
D = 2 // dimensionality
K = 3 // number of classes
X = np.zeros((N*K,D)) // data matrix (each row = single example)
y = np.zeros(N*K, dtype='uint8') // class labels
for j in xrange(K):
  ix = range(N*j,N*(j+1))
  r = np.linspace(0.0,1,N) // radius
  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 // theta
  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
  y[ix] = j
// lets visualize the data:
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)

// initialize parameters randomly
h = 100 // size of hidden layer
W = 0.01 * np.random.randn(D,h)
b = np.zeros((1,h))
W2 = 0.01 * np.random.randn(h,K)
b2 = np.zeros((1,K))

// some hyperparameters
step_size = 1e-0
reg = 1e-3 // regularization strength

// gradient descent loop
num_examples = X.shape[0]
for i in xrange(10000):
  
  // evaluate class scores, [N x K]
  hidden_layer = np.maximum(0, np.dot(X, W) + b) // note, ReLU activation
  scores = np.dot(hidden_layer, W2) + b2
  
  // compute the class probabilities
  exp_scores = np.exp(scores)
  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) // [N x K]
  
  // compute the loss: average cross-entropy loss and regularization
  corect_logprobs = -np.log(probs[range(num_examples),y])
  data_loss = np.sum(corect_logprobs)/num_examples
  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)
  loss = data_loss + reg_loss
  if i % 1000 == 0:
    print "iteration %d: loss %f" % (i, loss)
  
  // compute the gradient on scores
  dscores = probs
  dscores[range(num_examples),y] -= 1
  dscores /= num_examples
  
  // backpropate the gradient to the parameters
  // first backprop into parameters W2 and b2
  dW2 = np.dot(hidden_layer.T, dscores)
  db2 = np.sum(dscores, axis=0, keepdims=True)
  // next backprop into hidden layer
  dhidden = np.dot(dscores, W2.T)
  // backprop the ReLU non-linearity
  dhidden[hidden_layer <= 0] = 0
  // finally into W,b
  dW = np.dot(X.T, dhidden)
  db = np.sum(dhidden, axis=0, keepdims=True)
  
  // add regularization gradient contribution
  dW2 += reg * W2
  dW += reg * W
  
  // perform a parameter update
  W += -step_size * dW
  b += -step_size * db
  W2 += -step_size * dW2
  b2 += -step_size * db2
*/

/*
import numpy as np

// sigmoid function
def nonlin(x,deriv=False):
    if(deriv==True):
        return x*(1-x)
    return 1/(1+np.exp(-x))
    
// input dataset
X = np.array([  [0,0,1],
                [0,1,1],
                [1,0,1],
                [1,1,1] ])
    
// output dataset            
y = np.array([[0,0,1,1]]).T

// seed random numbers to make calculation
// deterministic (just a good practice)
np.random.seed(1)

// initialize weights randomly with mean 0
syn0 = 2*np.random.random((3,1)) - 1

for iter in xrange(10000):

    // forward propagation
    l0 = X
    l1 = nonlin(np.dot(l0,syn0))

    // how much did we miss?
    l1_error = y - l1

    // multiply how much we missed by the 
    // slope of the sigmoid at the values in l1
    l1_delta = l1_error * nonlin(l1,True)

    // update weights
    syn0 += np.dot(l0.T,l1_delta)

print "Output After Training:"
print l1


import numpy as np

def nonlin(x,deriv=False):
	if(deriv==True):
	    return x*(1-x)

	return 1/(1+np.exp(-x))
    
X = np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]])
                
y = np.array([[0],
			[1],
			[1],
			[0]])

np.random.seed(1)

// randomly initialize our weights with mean 0
syn0 = 2*np.random.random((3,4)) - 1
syn1 = 2*np.random.random((4,1)) - 1

for j in xrange(60000):

	// Feed forward through layers 0, 1, and 2
    l0 = X
    l1 = nonlin(np.dot(l0,syn0))
    l2 = nonlin(np.dot(l1,syn1))

    // how much did we miss the target value?
    l2_error = y - l2
    
    if (j% 10000) == 0:
        print "Error:" + str(np.mean(np.abs(l2_error)))
        
    // in what direction is the target value?
    // were we really sure? if so, don't change too much.
    l2_delta = l2_error*nonlin(l2,deriv=True)

    // how much did each l1 value contribute to the l2 error (according to the weights)?
    l1_error = l2_delta.dot(syn1.T)
    
    // in what direction is the target l1?
    // were we really sure? if so, don't change too much.
    l1_delta = l1_error * nonlin(l1,deriv=True)

    syn1 += l1.T.dot(l2_delta)
    syn0 += l0.T.dot(l1_delta)

*/

</script>